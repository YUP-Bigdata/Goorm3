***** Running training *****
  Num examples = 13538
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 33850
Loading model from checkpoints/checkpoint-430).
***** Running training *****
  Num examples = 13538
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 33850
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 430
  Will skip the first 0 epochs then the first 430 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
['괜찮지만 괜찮지 않아 익숙하다고 혼잣말했지만 늘 처음인 것처럼 아파', "I'm okay but I'm not okay I told myself I'm used to it But I'm in pain like it's the first time"]
['이제 함께 걸어요 조금은 어색하겠지만 이제 혼자가 아니에요 무거운 짐들 같이 들어 줄게요', 'Let’s walk together. It might be a little awkward I’m not alone anymore. I’ll carry heavy things with you']
PyTorch: setting up devices
Using amp half precision backend
Loading model from checkpoints/checkpoint-430).
/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 13538
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 33850
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 430
  Will skip the first 0 epochs then the first 430 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-3385
Configuration saved in checkpoints/checkpoint-3385/config.json
Model weights saved in checkpoints/checkpoint-3385/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-387] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-6770
Configuration saved in checkpoints/checkpoint-6770/config.json
Model weights saved in checkpoints/checkpoint-6770/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-3385] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-10155
Configuration saved in checkpoints/checkpoint-10155/config.json
Model weights saved in checkpoints/checkpoint-10155/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-6770] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-13540
Configuration saved in checkpoints/checkpoint-13540/config.json
Model weights saved in checkpoints/checkpoint-13540/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-430] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-16925
Configuration saved in checkpoints/checkpoint-16925/config.json
Model weights saved in checkpoints/checkpoint-16925/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-10155] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-20310
Configuration saved in checkpoints/checkpoint-20310/config.json
Model weights saved in checkpoints/checkpoint-20310/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-13540] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-23695
Configuration saved in checkpoints/checkpoint-23695/config.json
Model weights saved in checkpoints/checkpoint-23695/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-16925] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-27080
Configuration saved in checkpoints/checkpoint-27080/config.json
Model weights saved in checkpoints/checkpoint-27080/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-20310] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-30465
Configuration saved in checkpoints/checkpoint-30465/config.json
Model weights saved in checkpoints/checkpoint-30465/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-23695] due to args.save_total_limit
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
***** Running Evaluation *****
  Num examples = 13538
  Batch size = 4
Saving model checkpoint to checkpoints/checkpoint-33850
Configuration saved in checkpoints/checkpoint-33850/config.json
Model weights saved in checkpoints/checkpoint-33850/pytorch_model.bin
Deleting older checkpoint [checkpoints/checkpoint-27080] due to args.save_total_limit
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from checkpoints/checkpoint-33850 (score: 0.6589157581329346).
Configuration saved in checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model weights saved in checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
let it go let it go
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 놔 버려, 저 멀리멀리 날아 보자 놨어. 제발 놓아 버려. 나 눈도 못 맞추는구나, 한 번 더 말해봐. 가자, 오늘밤. 너와 나, 단둘이서.
1: 놔 버려, 저 멀리멀리 날아 보자 놨어. 제발 놓아 버려. 나 눈도 못 맞추는구나, 한 번 더 말해봐. 가자, 오늘밤. 너와 나, 단 둘이서 당신과 나
2: 놔 버려, 저 멀리멀리 날아 보자 놨어. 제발 놓아 버려. 나 눈도 못 맞추는구나, 한 번 더 말해봐. 가자, 오늘밤. 너와 나, 단둘이서. 날
3: 놔 버려, 저 멀리멀리 날아 보자 놨어. 제발 놓아 버려. 나 눈도 못 맞추는구나, 한 번 더 말해봐. 가자, 오늘밤. 너와 나, 단 둘이서 당신을
4: 놔 버려, 저 멀리멀리 날아 보자 놨어. 제발 놓아 버려. 나 눈도 못 맞추는구나, 한 번 더 말해봐. 가자, 오늘밤. 너와 나, 단 둘이서 당연하게 날
====================================================================================================
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
Hello, its me I was wondering if after all these years youd like to meet To go over everything They say that times supposed to heal ya But I aint done much healing
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 안녕, 나야 궁금했어... 오랜 시간이 지나면 다시 너를 만나서 지난 모든 것들을 되돌아 볼 수 있을지 사람들은 너에게 치유받길 바라는데 난 괜찮아지지가 않네, 하지만 난 잘 지내지 않아, 나는 아무 상처도 없는 거야... 너는 내가 어떻게 치유하는지 궁금해 하지만 나는 아직도 잘 살아왔냐고... 그건 정말 난 알아, 내가 잠깐 쉬는 것이었을 때... 웃을 수 있는 일은 없었다는 거였나? 너와 내가 함께 했던 모든 것이 다시 돌아올 거라는 것을 나는 확신했었어. 그때 넌 내가 어떤 표정을 짓고 있는지 알고 있었지 나는 그 모든 것을 다시 보고 싶어. 그래, 시간이 지나갈수록 나는 궁금해져. 너의 모든 것들이 다시 돌아온다는 것이 우리가 극복해야 할 것인지 사람들은 계속 치유되는 것이 뭔가? 나는 이해할 수 있겠지만, 나는 여전히... 나는 지난 세월을 지나야 한다고 말했지 그 시절을 되돌려야 할 거라고... 당신인가? 지금까지 쭉 내가 무슨 표정을 지어 봤는지 기억해 줘요? 그 지난 시간 속에서 나는 치유받는 것이 좀 낫기를 바라봤어요 하지만 전 알아요, 나는 지금까지 계속 치료받은 것을 되돌릴 수 있습니까? 그런 치유된 모든 것들이 다시 치유될 수 없습니다구요. 나는 아직 치료되지 않은 거예요 내가 뭘 치료받지 못한 것이 하나라고. 그러나 전 정말 나아졌으니까요... 그런 일은 없는데 하지만 안녕이라고. 안녕이었어요. 안녕... 지금은 좀 치료받으라는 말이야. 치료된 몸 한 해가 줬던 사람이 필요하긴 한 명이 필요해요! 하지만 지금은. 지금은 안녕. 하지만 치료받는 것은 끝났다고 말야 해요 다시 연락이 끊기면 안 되나 봐도 돼? 나에겐 상처는 것 같은 일이 아냐. 그게 인생에 대한 대답하지 않습니다. 항상 치유받은 일이 하나 있어 또 하나 있는데? 난 안녕입니다. 나는 안녕 제발 다시 한 번호로 돌아가고 있잖아? 하지만 내 인생에서 돌아온다고 말해 보던 것이야 뭐야? 안녕! 안녕? (아무뎌져 있네. 정말 다 치유하지 못한 일이 없더라? 내가 잘 살아가기 때문이야. 이제야 할지 모르겠어?" 내가 살아 왔으니? 정말 고맙다고? 또 안녕이다. 정말 진심이 났으니... 하지만 그 이름이 있냐? 다시 돌아왔다고! 이젠? 그리고 나는 행복하지 않아? 인생이란 말을 한 일이 끝난 일이 없어? 뭐든 좋으니까? 안녕이라 믿어보기엔
1: 안녕, 나야 궁금했어... 오랜 시간이 지나면 다시 너를 만나서 지난 모든 것들을 되돌아 볼 수 있을지 사람들은 너에게 치유받길 바라는데 난 괜찮아지지가 않네, 하지만 난 잘 지내지 않아, 나는 아무 상처도 없는 거야... 너는 내가 어떻게 치유하는지 궁금해 하지만 나는 아직도 잘 살아왔냐고... 그건 정말 난 알아, 내가 잠깐 쉬는 것이었을 때... 웃을 수 있는 일은 없었다는 거였나? 너와 내가 함께 했던 모든 것이 다시 돌아올 거라는 것을 나는 확신했었어. 그때 넌 내가 어떤 표정을 짓고 있는지 알고 있었지 나는 그 모든 것을 다시 보고 싶어. 그래, 시간이 지나갈수록 나는 궁금해져. 너의 모든 것들이 다시 돌아온다는 것이 우리가 극복해야 할 것인지 사람들은 계속 치유되는 것이 뭔가? 나는 이해할 수 있겠지만, 나는 여전히... 나는 지난 세월을 지나야 한다고 말했지 그 시절을 되돌려야 할 거라고... 당신인가? 지금까지 쭉 내가 무슨 표정을 지어 봤는지 기억해 줘요? 그 지난 시간 속에서 나는 치유받는 것이 좀 낫기를 바라봤어요 하지만 전 알아요, 나는 지금까지 계속 치료받은 것을 되돌릴 수 있습니까? 그런 치유된 모든 것들이 다시 치유될 수 없습니다구요. 나는 아직 치료되지 않은 거예요 내가 뭘 치료받지 못한 것이 하나라고. 그러나 전 정말 나아졌으니까요... 그런 일은 없는데 하지만 안녕이라고. 안녕이었어요. 안녕... 지금은 좀 치료받으라는 말이야. 치료된 몸 한 해가 줬던 사람이 필요하긴 한 명이 필요해요! 하지만 지금은. 지금은 안녕. 하지만 치료받는 것은 끝났다고 말야 해요 다시 연락이 끊기면 안 되나 봐도 돼? 나에겐 상처는 것 같은 일이 아냐. 그게 인생에 대한 대답하지 않습니다. 항상 치유받은 일이 하나 있어 또 하나 있는데? 난 안녕입니다. 나는 안녕 제발 다시 한 번호로 돌아가고 있잖아? 하지만 내 인생에서 돌아온다고 말해 보던 것이야 뭐야? 안녕! 안녕? (아무뎌져 있네. 정말 다 치유하지 못한 일이 없더라? 내가 잘 살아가기 때문이야. 이제야 할지 모르겠어?" 내가 살아 왔으니? 정말 고맙다고? 또 안녕이다. 정말 진심이 났으니... 하지만 그 이름이 있냐? 다시 돌아왔다고! 이젠? 그리고 나는 행복하지 않아? 인생이란 말을 한 일이 끝난 일이 없어? 뭐든 좋으니까? 안녕이라 믿어줘.
2: 안녕, 나야 궁금했어... 오랜 시간이 지나면 다시 너를 만나서 지난 모든 것들을 되돌아 볼 수 있을지 사람들은 너에게 치유받길 바라는데 난 괜찮아지지가 않네, 하지만 난 잘 지내지 않아, 나는 아무 상처도 없는 거야... 너는 내가 어떻게 치유하는지 궁금해 하지만 나는 아직도 잘 살아왔냐고... 그건 정말 난 알아, 내가 잠깐 쉬는 것이었을 때... 웃을 수 있는 일은 없었다는 거였나? 너와 내가 함께 했던 모든 것이 다시 돌아올 거라는 것을 나는 확신했었어. 그때 넌 내가 어떤 표정을 짓고 있는지 알고 있었지 나는 그 모든 것을 다시 보고 싶어. 그래, 시간이 지나갈수록 나는 궁금해져. 너의 모든 것들이 다시 돌아온다는 것이 우리가 극복해야 할 것인지 사람들은 계속 치유되는 것이 뭔가? 나는 이해할 수 있겠지만, 나는 여전히... 나는 지난 세월을 지나야 한다고 말했지 그 시절을 되돌려야 할 거라고... 당신인가? 지금까지 쭉 내가 무슨 표정을 지어 봤는지 기억해 줘요? 그 지난 시간 속에서 나는 치유받는 것이 좀 낫기를 바라봤어요 하지만 전 알아요, 나는 지금까지 계속 치료받은 것을 되돌릴 수 있습니까? 그런 치유된 모든 것들이 다시 치유될 수 없습니다구요. 나는 아직 치료되지 않은 거예요 내가 뭘 치료받지 못한 것이 하나라고. 그러나 전 정말 나아졌으니까요... 그런 일은 없는데 하지만 안녕이라고. 안녕이었어요. 안녕... 지금은 좀 치료받으라는 말이야. 치료된 몸 한 해가 줬던 사람이 필요하긴 한 명이 필요해요! 하지만 지금은. 지금은 안녕. 하지만 치료받는 것은 끝났다고 말야 해요 다시 연락이 끊기면 안 되나 봐도 돼? 나에겐 상처는 것 같은 일이 아냐. 그게 인생에 대한 대답하지 않습니다. 항상 치유받은 일이 하나 있어 또 하나 있는데? 난 안녕입니다. 나는 안녕 제발 다시 한 번호로 돌아가고 있잖아? 하지만 내 인생에서 돌아온다고 말해 보던 것이야 뭐야? 안녕! 안녕? (아무뎌져 있네. 정말 다 치유하지 못한 일이 없더라? 내가 잘 살아가기 때문이야. 이제야 할지 모르겠어?" 내가 살아 왔으니? 정말 고맙다고? 또 안녕이다. 정말 진심이 났으니... 하지만 그 이름이 있냐? 다시 돌아왔다고! 이젠? 그리고 나는 행복하지 않아? 인생이란 말을 한 일이 끝난 일이 없어? 뭐든 좋으니까? 안녕이라 믿어줘서
3: 안녕, 나야 궁금했어... 오랜 시간이 지나면 다시 너를 만나서 지난 모든 것들을 되돌아 볼 수 있을지 사람들은 너에게 치유받길 바라는데 난 괜찮아지지가 않네, 하지만 난 잘 지내지 않아, 나는 아무 상처도 없는 거야... 너는 내가 어떻게 치유하는지 궁금해 하지만 나는 아직도 잘 살아왔냐고... 그건 정말 난 알아, 내가 잠깐 쉬는 것이었을 때... 웃을 수 있는 일은 없었다는 거였나? 너와 내가 함께 했던 모든 것이 다시 돌아올 거라는 것을 나는 확신했었어. 그때 넌 내가 어떤 표정을 짓고 있는지 알고 있었지 나는 그 모든 것을 다시 보고 싶어. 그래, 시간이 지나갈수록 나는 궁금해져. 너의 모든 것들이 다시 돌아온다는 것이 우리가 극복해야 할 것인지 사람들은 계속 치유되는 것이 뭔가? 나는 이해할 수 있겠지만, 나는 여전히... 나는 지난 세월을 지나야 한다고 말했지 그 시절을 되돌려야 할 거라고... 당신인가? 지금까지 쭉 내가 무슨 표정을 지어 봤는지 기억해 줘요? 그 지난 시간 속에서 나는 치유받는 것이 좀 낫기를 바라봤어요 하지만 전 알아요, 나는 지금까지 계속 치료받은 것을 되돌릴 수 있습니까? 그런 치유된 모든 것들이 다시 치유될 수 없습니다구요. 나는 아직 치료되지 않은 거예요 내가 뭘 치료받지 못한 것이 하나라고. 그러나 전 정말 나아졌으니까요... 그런 일은 없는데 하지만 안녕이라고. 안녕이었어요. 안녕... 지금은 좀 치료받으라는 말이야. 치료된 몸 한 해가 줬던 사람이 필요하긴 한 명이 필요해요! 하지만 지금은. 지금은 안녕. 하지만 치료받는 것은 끝났다고 말야 해요 다시 연락이 끊기면 안 되나 봐도 돼? 나에겐 상처는 것 같은 일이 아냐. 그게 인생에 대한 대답하지 않습니다. 항상 치유받은 일이 하나 있어 또 하나 있는데? 난 안녕입니다. 나는 안녕 제발 다시 한 번호로 돌아가고 있잖아? 하지만 내 인생에서 돌아온다고 말해 보던 것이야 뭐야? 안녕! 안녕? (아무뎌져 있네. 정말 다 치유하지 못한 일이 없더라? 내가 잘 살아가기 때문이야. 이제야 할지 모르겠어?" 내가 살아 왔으니? 정말 고맙다고? 또 안녕이다. 정말 진심이 났으니... 하지만 그 이름이 있냐? 다시 돌아왔다고! 이젠? 그리고 나는 행복하지 않아? 인생이란 말을 한 일이 끝난 일이 없어? 뭐든 좋으니까? 안녕이라 믿어줘!
4: 안녕, 나야 궁금했어... 오랜 시간이 지나면 다시 너를 만나서 지난 모든 것들을 되돌아 볼 수 있을지 사람들은 너에게 치유받길 바라는데 난 괜찮아지지가 않네, 하지만 난 잘 지내지 않아, 나는 아무 상처도 없는 거야... 너는 내가 어떻게 치유하는지 궁금해 하지만 나는 아직도 잘 살아왔냐고... 그건 정말 난 알아, 내가 잠깐 쉬는 것이었을 때... 웃을 수 있는 일은 없었다는 거였나? 너와 내가 함께 했던 모든 것이 다시 돌아올 거라는 것을 나는 확신했었어. 그때 넌 내가 어떤 표정을 짓고 있는지 알고 있었지 나는 그 모든 것을 다시 보고 싶어. 그래, 시간이 지나갈수록 나는 궁금해져. 너의 모든 것들이 다시 돌아온다는 것이 우리가 극복해야 할 것인지 사람들은 계속 치유되는 것이 뭔가? 나는 이해할 수 있겠지만, 나는 여전히... 나는 지난 세월을 지나야 한다고 말했지 그 시절을 되돌려야 할 거라고... 당신인가? 지금까지 쭉 내가 무슨 표정을 지어 봤는지 기억해 줘요? 그 지난 시간 속에서 나는 치유받는 것이 좀 낫기를 바라봤어요 하지만 전 알아요, 나는 지금까지 계속 치료받은 것을 되돌릴 수 있습니까? 그런 치유된 모든 것들이 다시 치유될 수 없습니다구요. 나는 아직 치료되지 않은 거예요 내가 뭘 치료받지 못한 것이 하나라고. 그러나 전 정말 나아졌으니까요... 그런 일은 없는데 하지만 안녕이라고. 안녕이었어요. 안녕... 지금은 좀 치료받으라는 말이야. 치료된 몸 한 해가 줬던 사람이 필요하긴 한 명이 필요해요! 하지만 지금은. 지금은 안녕. 하지만 치료받는 것은 끝났다고 말야 해요 다시 연락이 끊기면 안 되나 봐도 돼? 나에겐 상처는 것 같은 일이 아냐. 그게 인생에 대한 대답하지 않습니다. 항상 치유받은 일이 하나 있어 또 하나 있는데? 난 안녕입니다. 나는 안녕 제발 다시 한 번호로 돌아가고 있잖아? 하지만 내 인생에서 돌아온다고 말해 보던 것이야 뭐야? 안녕! 안녕? (아무뎌져 있네. 정말 다 치유하지 못한 일이 없더라? 내가 잘 살아가기 때문이야. 이제야 할지 모르겠어?" 내가 살아 왔으니? 정말 고맙다고? 또 안녕이다. 정말 진심이 났으니... 하지만 그 이름이 있냐? 다시 돌아왔다고! 이젠? 그리고 나는 행복하지 않아? 인생이란 말을 한 일이 끝난 일이 없어? 뭐든 좋으니까? 안녕이라 믿어줘,
====================================================================================================
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
Aint no sunshine when shes gone Its not warm when shes away Aint no sunshine when shes gone And shes always gone too long Anytime shes goes away
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 그녀가 가버리면 그녀는 따뜻하지 않아 그녀가 떠났을 때 그녀가 떠나버렸을 때 그녀는 항상 너무 멀리 갔어 그녀는 그녀가 떠난 언제든 가겠다고 생각했어 그녀가 떠날 때마다 그녀는 계속 떠나간다고 여지껏 그리워했지 그녀는 떠나가면 그녀가 멀리 사라지지가 않는 어느 날 그녀는 떠났어 하지만 그녀는 여전히 어디에 있지도 못하지 내가 잘 안 그래, 그녀는 사라지지 않는 중이고 그게 내가 지우지는 않지 내가 원하는 건 내 사랑이 아닌데 왜 내가 그건지 기억이 안 나요 내가 잊을 수 없는 거라고 믿을 수 없어 내가 할 수 있는 건 아니지 이게 아니란걸 알아 넌 항상 멀리 떠나고 있어 그녀는 언제나 떠나가고 있지 않는데 이젠 집에 갈 때마다 나는 항상 떠나지 않을 거예요 그녀가 사라진다고 생각할 때 나는 언제나 떠날 때 그들은 항상 떠나갔다는 걸 알아 우우리가 떠나면 그녀는 절대 하지 않을거야 그녀가 갔다고 말할 때 그는 항상 떠났다고 믿을 수가 없어 그녀가 사라지면 그녀는 아무것도 아니야 내가 그렇게 살지 않는 게 아니에요 그리고 그녀가 간다고 할때 나는 모든 것이 사라지는 날마다 계속되지 않습니다 그녀가 나갔어요, 그녀는 당신이 떠난다고 언젠간 가 없는데 당신은 항상 그만두지 않을거에요 제가 모든 사랑이 사라진다는 것을 알아요 제발 내가 어떻게 한순간 멀어지겠어요 그녀는 아무 말이 없어요 저기 멀리 도망치고 있어요 전 모든 사람이 간다갔으니까요 그는 언제나 당신을 떠나갈 때마다 그녀가 사라져 버리기엔 정말 멀리 떠났다 갔다구요, 그녀가 가고 있죠 그녀는 사라져버리기만 해버릴때 그녀는 가버린다는 느낌이지가 않죠 그녀가 도망가면, 그녀는 매일 그리워할 때마다 그리고 그 여자가 간다면 그녀는 더이상 그 사람이 아닙니다라고 말야합니 우린 매일 속으로 사라졌어요. 그녀는 돌아가고 있거든요 여자가 가버리는 그 이름이 없는 일이 없을때 그녀가 돌아가면 나는 정말 나에겐 하루 종일때 괜찮지 않는 한 여자가 사라진다 아닙니다. 정말 정말 말이야 당신이라고 믿을 수도 없어 그녀는 사라진다면, 그녀는 그냥 가버리지는 않는 걸 안 좋아하지 않습니다. 항상 사랑이 사라집엔 사랑이 없거든요. 그녀가 가면 그녀는 정말 미안해요 그 모든 일이 아닐때문에선 안 예쁘지 않아 그녀는 도망가고 있잖아요 내겐 사랑이 사라지고 마십시오. 그녀가 돌아온다고 느낄 수 없는데도 항상 사라지기를 원했던 것이 아니죠 여자가 갔다는 것에 만족하지 않는 남자가 떠나버린다면 그냥 그런가 봐서 정말 그런가봐요, 여자가 떠나가나요 정말... 그녀가 갈
1: 그녀가 가버리면 그녀는 따뜻하지 않아 그녀가 떠났을 때 그녀가 떠나버렸을 때 그녀는 항상 너무 멀리 갔어 그녀는 그녀가 떠난 언제든 가겠다고 생각했어 그녀가 떠날 때마다 그녀는 계속 떠나간다고 여지껏 그리워했지 그녀는 떠나가면 그녀가 멀리 사라지지가 않는 어느 날 그녀는 떠났어 하지만 그녀는 여전히 어디에 있지도 못하지 내가 잘 안 그래, 그녀는 사라지지 않는 중이고 그게 내가 지우지는 않지 내가 원하는 건 내 사랑이 아닌데 왜 내가 그건지 기억이 안 나요 내가 잊을 수 없는 거라고 믿을 수 없어 내가 할 수 있는 건 아니지 이게 아니란걸 알아 넌 항상 멀리 떠나고 있어 그녀는 언제나 떠나가고 있지 않는데 이젠 집에 갈 때마다 나는 항상 떠나지 않을 거예요 그녀가 사라진다고 생각할 때 나는 언제나 떠날 때 그들은 항상 떠나갔다는 걸 알아 우우리가 떠나면 그녀는 절대 하지 않을거야 그녀가 갔다고 말할 때 그는 항상 떠났다고 믿을 수가 없어 그녀가 사라지면 그녀는 아무것도 아니야 내가 그렇게 살지 않는 게 아니에요 그리고 그녀가 간다고 할때 나는 모든 것이 사라지는 날마다 계속되지 않습니다 그녀가 나갔어요, 그녀는 당신이 떠난다고 언젠간 가 없는데 당신은 항상 그만두지 않을거에요 제가 모든 사랑이 사라진다는 것을 알아요 제발 내가 어떻게 한순간 멀어지겠어요 그녀는 아무 말이 없어요 저기 멀리 도망치고 있어요 전 모든 사람이 간다갔으니까요 그는 언제나 당신을 떠나갈 때마다 그녀가 사라져 버리기엔 정말 멀리 떠났다 갔다구요, 그녀가 가고 있죠 그녀는 사라져버리기만 해버릴때 그녀는 가버린다는 느낌이지가 않죠 그녀가 도망가면, 그녀는 매일 그리워할 때마다 그리고 그 여자가 간다면 그녀는 더이상 그 사람이 아닙니다라고 말야합니 우린 매일 속으로 사라졌어요. 그녀는 돌아가고 있거든요 여자가 가버리는 그 이름이 없는 일이 없을때 그녀가 돌아가면 나는 정말 나에겐 하루 종일때 괜찮지 않는 한 여자가 사라진다 아닙니다. 정말 정말 말이야 당신이라고 믿을 수도 없어 그녀는 사라진다면, 그녀는 그냥 가버리지는 않는 걸 안 좋아하지 않습니다. 항상 사랑이 사라집엔 사랑이 없거든요. 그녀가 가면 그녀는 정말 미안해요 그 모든 일이 아닐때문에선 안 예쁘지 않아 그녀는 도망가고 있잖아요 내겐 사랑이 사라지고 마십시오. 그녀가 돌아온다고 느낄 수 없는데도 항상 사라지기를 원했던 것이 아니죠 여자가 갔다는 것에 만족하지 않는 남자가 떠나버린다면 그냥 그런가 봐서 정말 그런가봐요, 여자가 떠나가나요 정말... 물론엔
2: 그녀가 가버리면 그녀는 따뜻하지 않아 그녀가 떠났을 때 그녀가 떠나버렸을 때 그녀는 항상 너무 멀리 갔어 그녀는 그녀가 떠난 언제든 가겠다고 생각했어 그녀가 떠날 때마다 그녀는 계속 떠나간다고 여지껏 그리워했지 그녀는 떠나가면 그녀가 멀리 사라지지가 않는 어느 날 그녀는 떠났어 하지만 그녀는 여전히 어디에 있지도 못하지 내가 잘 안 그래, 그녀는 사라지지 않는 중이고 그게 내가 지우지는 않지 내가 원하는 건 내 사랑이 아닌데 왜 내가 그건지 기억이 안 나요 내가 잊을 수 없는 거라고 믿을 수 없어 내가 할 수 있는 건 아니지 이게 아니란걸 알아 넌 항상 멀리 떠나고 있어 그녀는 언제나 떠나가고 있지 않는데 이젠 집에 갈 때마다 나는 항상 떠나지 않을 거예요 그녀가 사라진다고 생각할 때 나는 언제나 떠날 때 그들은 항상 떠나갔다는 걸 알아 우우리가 떠나면 그녀는 절대 하지 않을거야 그녀가 갔다고 말할 때 그는 항상 떠났다고 믿을 수가 없어 그녀가 사라지면 그녀는 아무것도 아니야 내가 그렇게 살지 않는 게 아니에요 그리고 그녀가 간다고 할때 나는 모든 것이 사라지는 날마다 계속되지 않습니다 그녀가 나갔어요, 그녀는 당신이 떠난다고 언젠간 가 없는데 당신은 항상 그만두지 않을거에요 제가 모든 사랑이 사라진다는 것을 알아요 제발 내가 어떻게 한순간 멀어지겠어요 그녀는 아무 말이 없어요 저기 멀리 도망치고 있어요 전 모든 사람이 간다갔으니까요 그는 언제나 당신을 떠나갈 때마다 그녀가 사라져 버리기엔 정말 멀리 떠났다 갔다구요, 그녀가 가고 있죠 그녀는 사라져버리기만 해버릴때 그녀는 가버린다는 느낌이지가 않죠 그녀가 도망가면, 그녀는 매일 그리워할 때마다 그리고 그 여자가 간다면 그녀는 더이상 그 사람이 아닙니다라고 말야합니 우린 매일 속으로 사라졌어요. 그녀는 돌아가고 있거든요 여자가 가버리는 그 이름이 없는 일이 없을때 그녀가 돌아가면 나는 정말 나에겐 하루 종일때 괜찮지 않는 한 여자가 사라진다 아닙니다. 정말 정말 말이야 당신이라고 믿을 수도 없어 그녀는 사라진다면, 그녀는 그냥 가버리지는 않는 걸 안 좋아하지 않습니다. 항상 사랑이 사라집엔 사랑이 없거든요. 그녀가 가면 그녀는 정말 미안해요 그 모든 일이 아닐때문에선 안 예쁘지 않아 그녀는 도망가고 있잖아요 내겐 사랑이 사라지고 마십시오. 그녀가 돌아온다고 느낄 수 없는데도 항상 사라지기를 원했던 것이 아니죠 여자가 갔다는 것에 만족하지 않는 남자가 떠나버린다면 그냥 그런가 봐서 정말 그런가봐요, 여자가 떠나가나요 정말... 물론 당
3: 그녀가 가버리면 그녀는 따뜻하지 않아 그녀가 떠났을 때 그녀가 떠나버렸을 때 그녀는 항상 너무 멀리 갔어 그녀는 그녀가 떠난 언제든 가겠다고 생각했어 그녀가 떠날 때마다 그녀는 계속 떠나간다고 여지껏 그리워했지 그녀는 떠나가면 그녀가 멀리 사라지지가 않는 어느 날 그녀는 떠났어 하지만 그녀는 여전히 어디에 있지도 못하지 내가 잘 안 그래, 그녀는 사라지지 않는 중이고 그게 내가 지우지는 않지 내가 원하는 건 내 사랑이 아닌데 왜 내가 그건지 기억이 안 나요 내가 잊을 수 없는 거라고 믿을 수 없어 내가 할 수 있는 건 아니지 이게 아니란걸 알아 넌 항상 멀리 떠나고 있어 그녀는 언제나 떠나가고 있지 않는데 이젠 집에 갈 때마다 나는 항상 떠나지 않을 거예요 그녀가 사라진다고 생각할 때 나는 언제나 떠날 때 그들은 항상 떠나갔다는 걸 알아 우우리가 떠나면 그녀는 절대 하지 않을거야 그녀가 갔다고 말할 때 그는 항상 떠났다고 믿을 수가 없어 그녀가 사라지면 그녀는 아무것도 아니야 내가 그렇게 살지 않는 게 아니에요 그리고 그녀가 간다고 할때 나는 모든 것이 사라지는 날마다 계속되지 않습니다 그녀가 나갔어요, 그녀는 당신이 떠난다고 언젠간 가 없는데 당신은 항상 그만두지 않을거에요 제가 모든 사랑이 사라진다는 것을 알아요 제발 내가 어떻게 한순간 멀어지겠어요 그녀는 아무 말이 없어요 저기 멀리 도망치고 있어요 전 모든 사람이 간다갔으니까요 그는 언제나 당신을 떠나갈 때마다 그녀가 사라져 버리기엔 정말 멀리 떠났다 갔다구요, 그녀가 가고 있죠 그녀는 사라져버리기만 해버릴때 그녀는 가버린다는 느낌이지가 않죠 그녀가 도망가면, 그녀는 매일 그리워할 때마다 그리고 그 여자가 간다면 그녀는 더이상 그 사람이 아닙니다라고 말야합니 우린 매일 속으로 사라졌어요. 그녀는 돌아가고 있거든요 여자가 가버리는 그 이름이 없는 일이 없을때 그녀가 돌아가면 나는 정말 나에겐 하루 종일때 괜찮지 않는 한 여자가 사라진다 아닙니다. 정말 정말 말이야 당신이라고 믿을 수도 없어 그녀는 사라진다면, 그녀는 그냥 가버리지는 않는 걸 안 좋아하지 않습니다. 항상 사랑이 사라집엔 사랑이 없거든요. 그녀가 가면 그녀는 정말 미안해요 그 모든 일이 아닐때문에선 안 예쁘지 않아 그녀는 도망가고 있잖아요 내겐 사랑이 사라지고 마십시오. 그녀가 돌아온다고 느낄 수 없는데도 항상 사라지기를 원했던 것이 아니죠 여자가 갔다는 것에 만족하지 않는 남자가 떠나버린다면 그냥 그런가 봐서 정말 그런가봐요. 정말... 그녀가 지나갔어. 그녀는
4: 그녀가 가버리면 그녀는 따뜻하지 않아 그녀가 떠났을 때 그녀가 떠나버렸을 때 그녀는 항상 너무 멀리 갔어 그녀는 그녀가 떠난 언제든 가겠다고 생각했어 그녀가 떠날 때마다 그녀는 계속 떠나간다고 여지껏 그리워했지 그녀는 떠나가면 그녀가 멀리 사라지지가 않는 어느 날 그녀는 떠났어 하지만 그녀는 여전히 어디에 있지도 못하지 내가 잘 안 그래, 그녀는 사라지지 않는 중이고 그게 내가 지우지는 않지 내가 원하는 건 내 사랑이 아닌데 왜 내가 그건지 기억이 안 나요 내가 잊을 수 없는 거라고 믿을 수 없어 내가 할 수 있는 건 아니지 이게 아니란걸 알아 넌 항상 멀리 떠나고 있어 그녀는 언제나 떠나가고 있지 않는데 이젠 집에 갈 때마다 나는 항상 떠나지 않을 거예요 그녀가 사라진다고 생각할 때 나는 언제나 떠날 때 그들은 항상 떠나갔다는 걸 알아 우우리가 떠나면 그녀는 절대 하지 않을거야 그녀가 갔다고 말할 때 그는 항상 떠났다고 믿을 수가 없어 그녀가 사라지면 그녀는 아무것도 아니야 내가 그렇게 살지 않는 게 아니에요 그리고 그녀가 간다고 할때 나는 모든 것이 사라지는 날마다 계속되지 않습니다 그녀가 나갔어요, 그녀는 당신이 떠난다고 언젠간 가 없는데 당신은 항상 그만두지 않을거에요 제가 모든 사랑이 사라진다는 것을 알아요 제발 내가 어떻게 한순간 멀어지겠어요 그녀는 아무 말이 없어요 저기 멀리 도망치고 있어요 전 모든 사람이 간다갔으니까요 그는 언제나 당신을 떠나갈 때마다 그녀가 사라져 버리기엔 정말 멀리 떠났다 갔다구요, 그녀가 가고 있죠 그녀는 사라져버리기만 해버릴때 그녀는 가버린다는 느낌이지가 않죠 그녀가 도망가면, 그녀는 매일 그리워할 때마다 그리고 그 여자가 간다면 그녀는 더이상 그 사람이 아닙니다라고 말야합니 우린 매일 속으로 사라졌어요. 그녀는 돌아가고 있거든요 여자가 가버리는 그 이름이 없는 일이 없을때 그녀가 돌아가면 나는 정말 나에겐 하루 종일때 괜찮지 않는 한 여자가 사라진다 아닙니다. 정말 정말 말이야 당신이라고 믿을 수도 없어 그녀는 사라진다면, 그녀는 그냥 가버리지는 않는 걸 안 좋아하지 않습니다. 항상 사랑이 사라집엔 사랑이 없거든요. 그녀가 가면 그녀는 정말 미안해요 그 모든 일이 아닐때문에선 안 예쁘지 않아 그녀는 도망가고 있잖아요 내겐 사랑이 사라지고 마십시오. 그녀가 돌아온다고 느낄 수 없는데도 항상 사라지기를 원했던 것이 아니죠 여자가 갔다는 것에 만족하지 않는 남자가 떠나버린다면 그냥 그런가 봐서 정말 그런가봐요, 여자가 떠나가나요 정말... 그녀가 원하는
====================================================================================================
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
You try to hold me down so I became a soldier Built up all theses walls and now Im climbing over Those nasty bees are tempting me Oh lord, but I aint going back
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 너는 날 눌렀어 나는 병사이 됐어 이 모든 벽에 쌓였지 이제 나는 그 썩은 벌레들을 넘고 있지 오 신이시여, 제가 돌아갈 수 없나봐요 하나님, 제발 돌아가진 않을래요 하지만 전 다시 돌아갈 순 없어요 저는 아무 것도 가지고 있지 않으니까요 오, 나는 돌아오지 않을 거야 나는 돌아가려고 노력했어요 그러면 나는 쓰러지지 않겠지만 나는 다시 돌아올 거예요 그 빌어먹을 수 있는 모든 요정들이 저를 끌어내리고 있어요 전 너무 때렸어요, 전 떠나지 않으면 안 돼요 제가 돌아올 용기가 되길 바라면서요 이런저런 노력들을 실패는 하지 않았어 그리고 저가 날 무릎을 꿇어 넘기는 요정이 되니 전사가 돼주었죠 그러니 저의 모든 귀여운 벌레들이 제게 등을 올려놓았어 그래서 나는 이제 다시 돌아서가지 않으려 해 줘요, 이 지옥에 갔다구요 그리고 제가 살아있다는 것을 알게 되짚어 볼게요 이 나쁜 것들이 나를 살리고 있지 않을까봐 요정을 넘어버려 봤다고 다시 돌이킬 수 없어요, 난 돌아갈거야 전활 걸어오세요 그럼 나는 돌아왔다고 해요 아니 전 모든 악마들은 나를 떨쳐내리려고 해봤다 저한테서 도망치지 않습니다시는 맙소원해요 하지만, 전 이제 재탈은 하지 않아요 난 다시 돌아온다고 말해주세요, 오 그대 넌 저와 함께 있으신전처럼 돌아가고 있으리라구요. 전처럼 전다시 돌아갈 생각으시고 싶어요 제 인생에서 부활을 포기해보리지만, 전 끝까지 그만두지 않으셔도 아니꼬리를 포기하진 않겠죠 하지만 난 돌아가버리기 위해선 나는 돌아갈 곳은 없네요, 나는 결코 포기하지 않아요, 아니겠지만 전 그냥 포기하기만 하실래요, 아냐, 전우야. 하지만 다시 돌아가렴 헤매야 해요, 하셔봐요. 이런 말하는 대로 멈추지 마셔. 이제 전 정말 그런가봐요, 제대론 재가 내려오지 말아주겠지마요. 돌아가기 위해 제가 다시 갈 순 없다니 당신을 떨쳐내기 위한 삶으라. 전 좀 도와주려오. 전심으로! 전 돌아가 버텨봐도 하시기전에선 팔에겐 내가 살아갈 거니까요! 이 못하리라는 말씀대로 걸어갈 수 있잖아니? 전 믿어주겠어요. 그리고 전 제버리지 않다니까요. 이제 나도 돌아가버리진 않네요? 다시 돌아오기 바라보기 위해
1: 너는 날 눌렀어 나는 병사이 됐어 이 모든 벽에 쌓였지 이제 나는 그 썩은 벌레들을 넘고 있지 오 신이시여, 제가 돌아갈 수 없나봐요 하나님, 제발 돌아가진 않을래요 하지만 전 다시 돌아갈 순 없어요 저는 아무 것도 가지고 있지 않으니까요 오, 나는 돌아오지 않을 거야 나는 돌아가려고 노력했어요 그러면 나는 쓰러지지 않겠지만 나는 다시 돌아올 거예요 그 빌어먹을 수 있는 모든 요정들이 저를 끌어내리고 있어요 전 너무 때렸어요, 전 떠나지 않으면 안 돼요 제가 돌아올 용기가 되길 바라면서요 이런저런 노력들을 실패는 하지 않았어 그리고 저가 날 무릎을 꿇어 넘기는 요정이 되니 전사가 돼주었죠 그러니 저의 모든 귀여운 벌레들이 제게 등을 올려놓았어 그래서 나는 이제 다시 돌아서가지 않으려 해 줘요, 이 지옥에 갔다구요 그리고 제가 살아있다는 것을 알게 되짚어 볼게요 이 나쁜 것들이 나를 살리고 있지 않을까봐 요정을 넘어버려 봤다고 다시 돌이킬 수 없어요, 난 돌아갈거야 전활 걸어오세요 그럼 나는 돌아왔다고 해요 아니 전 모든 악마들은 나를 떨쳐내리려고 해봤다 저한테서 도망치지 않습니다시는 맙소원해요 하지만, 전 이제 재탈은 하지 않아요 난 다시 돌아온다고 말해주세요, 오 그대 넌 저와 함께 있으신전처럼 돌아가고 있으리라구요. 전처럼 전다시 돌아갈 생각으시고 싶어요 제 인생에서 부활을 포기해보리지만, 전 끝까지 그만두지 않으셔도 아니꼬리를 포기하진 않겠죠 하지만 난 돌아가버리기 위해선 나는 돌아갈 곳은 없네요, 나는 결코 포기하지 않아요, 아니겠지만 전 그냥 포기하기만 하실래요, 아냐, 전우야. 하지만 다시 돌아가렴 헤매야 해요, 하셔봐요. 이런 말하는 대로 멈추지 마셔. 이제 전 정말 그런가봐요, 제대론 재가 내려오지 말아주겠지마요. 돌아가기 위해 제가 다시 갈 순 없다니 당신을 떨쳐내기 위한 삶으라. 전 좀 도와주려오. 전심으로! 전 돌아가 버텨봐도 하시기전에선 팔에겐 내가 살아갈 거니까요! 이 못하리라는 말씀대로 걸어갈 수 있잖아니? 전 믿어주겠어요. 그리고 전 제버리지 않다니까요. 이제 나도 돌아가버리진 않네요? 다시 돌아오기 바라다보겠
2: 너는 날 눌렀어 나는 병사이 됐어 이 모든 벽에 쌓였지 이제 나는 그 썩은 벌레들을 넘고 있지 오 신이시여, 제가 돌아갈 수 없나봐요 하나님, 제발 돌아가진 않을래요 하지만 전 다시 돌아갈 순 없어요 저는 아무 것도 가지고 있지 않으니까요 오, 나는 돌아오지 않을 거야 나는 돌아가려고 노력했어요 그러면 나는 쓰러지지 않겠지만 나는 다시 돌아올 거예요 그 빌어먹을 수 있는 모든 요정들이 저를 끌어내리고 있어요 전 너무 때렸어요, 전 떠나지 않으면 안 돼요 제가 돌아올 용기가 되길 바라면서요 이런저런 노력들을 실패는 하지 않았어 그리고 저가 날 무릎을 꿇어 넘기는 요정이 되니 전사가 돼주었죠 그러니 저의 모든 귀여운 벌레들이 제게 등을 올려놓았어 그래서 나는 이제 다시 돌아서가지 않으려 해 줘요, 이 지옥에 갔다구요 그리고 제가 살아있다는 것을 알게 되짚어 볼게요 이 나쁜 것들이 나를 살리고 있지 않을까봐 요정을 넘어버려 봤다고 다시 돌이킬 수 없어요, 난 돌아갈거야 전활 걸어오세요 그럼 나는 돌아왔다고 해요 아니 전 모든 악마들은 나를 떨쳐내리려고 해봤다 저한테서 도망치지 않습니다시는 맙소원해요 하지만, 전 이제 재탈은 하지 않아요 난 다시 돌아온다고 말해주세요, 오 그대 넌 저와 함께 있으신전처럼 돌아가고 있으리라구요. 전처럼 전다시 돌아갈 생각으시고 싶어요 제 인생에서 부활을 포기해보리지만, 전 끝까지 그만두지 않으셔도 아니꼬리를 포기하진 않겠죠 하지만 난 돌아가버리기 위해선 나는 돌아갈 곳은 없네요, 나는 결코 포기하지 않아요, 아니겠지만 전 그냥 포기하기만 하실래요, 아냐, 전우야. 하지만 다시 돌아가렴 헤매야 해요, 하셔봐요. 이런 말하는 대로 멈추지 마셔. 이제 전 정말 그런가봐요, 제대론 재가 내려오지 말아주겠지마요. 돌아가기 위해 제가 다시 갈 순 없다니 당신을 떨쳐내기 위한 삶으라. 전 좀 도와주려오. 전심으로! 전 돌아가 버텨봐도 하시기전에선 팔에겐 내가 살아갈 거니까요! 이 못하리라는 말씀대로 걸어갈 수 있잖아니? 전 믿어주겠어요. 그리고 전 제버리지 않다니까요. 이제 나도 돌아가버리진 않네요? 다시 돌아오기 바라다보니
3: 너는 날 눌렀어 나는 병사이 됐어 이 모든 벽에 쌓였지 이제 나는 그 썩은 벌레들을 넘고 있지 오 신이시여, 제가 돌아갈 수 없나봐요 하나님, 제발 돌아가진 않을래요 하지만 전 다시 돌아갈 순 없어요 저는 아무 것도 가지고 있지 않으니까요 오, 나는 돌아오지 않을 거야 나는 돌아가려고 노력했어요 그러면 나는 쓰러지지 않겠지만 나는 다시 돌아올 거예요 그 빌어먹을 수 있는 모든 요정들이 저를 끌어내리고 있어요 전 너무 때렸어요, 전 떠나지 않으면 안 돼요 제가 돌아올 용기가 되길 바라면서요 이런저런 노력들을 실패는 하지 않았어 그리고 저가 날 무릎을 꿇어 넘기는 요정이 되니 전사가 돼주었죠 그러니 저의 모든 귀여운 벌레들이 제게 등을 올려놓았어 그래서 나는 이제 다시 돌아서가지 않으려 해 줘요, 이 지옥에 갔다구요 그리고 제가 살아있다는 것을 알게 되짚어 볼게요 이 나쁜 것들이 나를 살리고 있지 않을까봐 요정을 넘어버려 봤다고 다시 돌이킬 수 없어요, 난 돌아갈거야 전활 걸어오세요 그럼 나는 돌아왔다고 해요 아니 전 모든 악마들은 나를 떨쳐내리려고 해봤다 저한테서 도망치지 않습니다시는 맙소원해요 하지만, 전 이제 재탈은 하지 않아요 난 다시 돌아온다고 말해주세요, 오 그대 넌 저와 함께 있으신전처럼 돌아가고 있으리라구요. 전처럼 전다시 돌아갈 생각으시고 싶어요 제 인생에서 부활을 포기해보리지만, 전 끝까지 그만두지 않으셔도 아니꼬리를 포기하진 않겠죠 하지만 난 돌아가버리기 위해선 나는 돌아갈 곳은 없네요, 나는 결코 포기하지 않아요, 아니겠지만 전 그냥 포기하기만 하실래요, 아냐, 전우야. 하지만 다시 돌아가렴 헤매야 해요, 하셔봐요. 이런 말하는 대로 멈추지 마셔. 이제 전 정말 그런가봐요, 제대론 재가 내려오지 말아주겠지마요. 돌아가기 위해 제가 다시 갈 순 없다니 당신을 떨쳐내기 위한 삶으라. 전 좀 도와주려오. 전심으로! 전 돌아가 버텨봐도 하시기전에선 팔에겐 내가 살아갈 거니까요! 이 못하리라는 말씀대로 걸어갈 수 있잖아니? 전 믿어주겠어요. 그리고 전 제버리지 않다니까요. 이제 나도 돌아가버리진 않네요? 다시 돌아오기 바라던 저
4: 너는 날 눌렀어 나는 병사이 됐어 이 모든 벽에 쌓였지 이제 나는 그 썩은 벌레들을 넘고 있지 오 신이시여, 제가 돌아갈 수 없나봐요 하나님, 제발 돌아가진 않을래요 하지만 전 다시 돌아갈 순 없어요 저는 아무 것도 가지고 있지 않으니까요 오, 나는 돌아오지 않을 거야 나는 돌아가려고 노력했어요 그러면 나는 쓰러지지 않겠지만 나는 다시 돌아올 거예요 그 빌어먹을 수 있는 모든 요정들이 저를 끌어내리고 있어요 전 너무 때렸어요, 전 떠나지 않으면 안 돼요 제가 돌아올 용기가 되길 바라면서요 이런저런 노력들을 실패는 하지 않았어 그리고 저가 날 무릎을 꿇어 넘기는 요정이 되니 전사가 돼주었죠 그러니 저의 모든 귀여운 벌레들이 제게 등을 올려놓았어 그래서 나는 이제 다시 돌아서가지 않으려 해 줘요, 이 지옥에 갔다구요 그리고 제가 살아있다는 것을 알게 되짚어 볼게요 이 나쁜 것들이 나를 살리고 있지 않을까봐 요정을 넘어버려 봤다고 다시 돌이킬 수 없어요, 난 돌아갈거야 전활 걸어오세요 그럼 나는 돌아왔다고 해요 아니 전 모든 악마들은 나를 떨쳐내리려고 해봤다 저한테서 도망치지 않습니다시는 맙소원해요 하지만, 전 이제 재탈은 하지 않아요 난 다시 돌아온다고 말해주세요, 오 그대 넌 저와 함께 있으신전처럼 돌아가고 있으리라구요. 전처럼 전다시 돌아갈 생각으시고 싶어요 제 인생에서 부활을 포기해보리지만, 전 끝까지 그만두지 않으셔도 아니꼬리를 포기하진 않겠죠 하지만 난 돌아가버리기 위해선 나는 돌아갈 곳은 없네요, 나는 결코 포기하지 않아요, 아니겠지만 전 그냥 포기하기만 하실래요, 아냐, 전우야. 하지만 다시 돌아가렴 헤매야 해요, 하셔봐요. 이런 말하는 대로 멈추지 마셔. 이제 전 정말 그런가봐요, 제대론 재가 내려오지 말아주겠지마요. 돌아가기 위해 제가 다시 갈 순 없다니 당신을 떨쳐내기 위한 삶으라. 전 좀 도와주려오. 전심으로! 전 돌아가 버텨봐도 하시기전에선 팔에겐 내가 살아갈 거니까요! 이 못하리라는 말씀대로 걸어갈 수 있잖아니? 전 믿어주겠어요. 그리고 전 제버리지 않다니까요. 이제 나도 돌아가버리진 않네요? 다시 돌아오기 바라보기엔
====================================================================================================
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
I had a bad week Spent the evening pretending it wasnt that deep You could see in my eyes that it was taking over I guess I was just blind and caught up
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 난 나쁜 하루를 보냈지 그 밤은 깊지 않은 척 넌 알 수 있는 내 눈에서 그게 보일까 봐 난 눈이 멀었나 봐 그냥 멍청했었지 내가 좀 봐서 눈치도 안 빠질 것 같아 아마 내겐 들리지 않는걸 나 혼자서 괜히 애태우는 건 좀 약이었나봐 잠시 스쳐가는 인연이길 바랬어 아픈 상처만 내게 남겨진다는 거짓말 난 그냥 참아내는 건데 바보 같은 남자라던 날 안아주는 건 이제야 넋이 나가 숨 쉬는 순간 정신을 못 차리고 알았어 난 그저 눈앞을 맴돌았지 점점 내 눈에 멀어져 가는걸 아무 의미 없단 걸 알게 됐어 그냥 미친 척했지 하긴 했지 뭔가 빠진 것처럼 날 꼬셔버린 시간을 버텨고 버틸 수 있게 된거야 난 그렇게 눈치를 챘던 짓궂은 장난말만 했었어 하지만 그걸 어쩌다 버렸어 제발 날 용서해 줬던 네가 눈가에 그저 눈빛이 보여 마치 내 마음속 깊이 박힌 눈빛으로 날 아프게 한 켠에 앞이 보이던 사람이 바로 너란걸 내가 다 봤어 우린 그렇게 난 눈치가 안 났지 그저 곁에 있잖아 나는 좀 미쳐버렸지 좀 늦은 저녁을 보냈던 중이었지 다신 볼 수 없게 널 그만 붙잡고버려 뒀지 난 좀 미쳤다 다시 끌려왔지 제자리에 멈춰서 버렸던 거야 나 혼자 죗악몽에 빠진 것 같은 짓을 했어 다뤘지 또 내 탓이겠지 되버린 탓이라며 날 미치게 보고 있는 게 더 나쁜 짓밟아가는 건 못됐지 나 스스로 다 너와 내 인생 최악의 모습인거지 말해본 적도 없는데 그냥 나쁜 모습 자고 또 다시 돌아온다는 말만 해본적이었어 그저 내버렸던 인생이야 다시 돌아올 거라며 날 버튼 박힌 잔인한 척 했다 나쁘게 보내며 계속 너를 보내주던 대로 걸어가는 패놓고 나만 있었어 정말 못되게 자빠진 않으니까 무뎌졌어 아무래도 내 길을 걸었다 고갤 떨리는 건 모두 다 잊고 있더라고 말야 진심해 난 좋은 일만 없었지 뭐가면 돼버린 시간만 또 끌려 보낸적이라던 내가 잘 버틴 게 나아졌지려다던 지난 밤새 술 한숨이였어 좋은 남자애태우린 사람 못자며 지내셨지
1: 난 나쁜 하루를 보냈지 그 밤은 깊지 않은 척 넌 알 수 있는 내 눈에서 그게 보일까 봐 난 눈이 멀었나 봐 그냥 멍청했었지 내가 좀 봐서 눈치도 안 빠질 것 같아 아마 내겐 들리지 않는걸 나 혼자서 괜히 애태우는 건 좀 약이었나봐 잠시 스쳐가는 인연이길 바랬어 아픈 상처만 내게 남겨진다는 거짓말 난 그냥 참아내는 건데 바보 같은 남자라던 날 안아주는 건 이제야 넋이 나가 숨 쉬는 순간 정신을 못 차리고 알았어 난 그저 눈앞을 맴돌았지 점점 내 눈에 멀어져 가는걸 아무 의미 없단 걸 알게 됐어 그냥 미친 척했지 하긴 했지 뭔가 빠진 것처럼 날 꼬셔버린 시간을 버텨고 버틸 수 있게 된거야 난 그렇게 눈치를 챘던 짓궂은 장난말만 했었어 하지만 그걸 어쩌다 버렸어 제발 날 용서해 줬던 네가 눈가에 그저 눈빛이 보여 마치 내 마음속 깊이 박힌 눈빛으로 날 아프게 한 켠에 앞이 보이던 사람이 바로 너란걸 내가 다 봤어 우린 그렇게 난 눈치가 안 났지 그저 곁에 있잖아 나는 좀 미쳐버렸지 좀 늦은 저녁을 보냈던 중이었지 다신 볼 수 없게 널 그만 붙잡고버려 뒀지 난 좀 미쳤다 다시 끌려왔지 제자리에 멈춰서 버렸던 거야 나 혼자 죗악몽에 빠진 것 같은 짓을 했어 다뤘지 또 내 탓이겠지 되버린 탓이라며 날 미치게 보고 있는 게 더 나쁜 짓밟아가는 건 못됐지 나 스스로 다 너와 내 인생 최악의 모습인거지 말해본 적도 없는데 그냥 나쁜 모습 자고 또 다시 돌아온다는 말만 해본적이었어 그저 내버렸던 인생이야 다시 돌아올 거라며 날 버튼 박힌 잔인한 척 했다 나쁘게 보내며 계속 너를 보내주던 대로 걸어가는 패놓고 나만 있었어 정말 못되게 자빠진 않으니까 무뎌졌어 아무래도 내 길을 걸었다 고갤 떨리는 건 모두 다 잊고 있더라고 말야 진심해 난 좋은 일만 없었지 뭐가면 돼버린 시간만 또 끌려 보낸적이라던 내가 잘 버틴 게 나아졌지려다던 지난 밤새 술 한숨이였어 좋은 남자애태우린 사람 못자며 지내셨어
2: 난 나쁜 하루를 보냈지 그 밤은 깊지 않은 척 넌 알 수 있는 내 눈에서 그게 보일까 봐 난 눈이 멀었나 봐 그냥 멍청했었지 내가 좀 봐서 눈치도 안 빠질 것 같아 아마 내겐 들리지 않는걸 나 혼자서 괜히 애태우는 건 좀 약이었나봐 잠시 스쳐가는 인연이길 바랬어 아픈 상처만 내게 남겨진다는 거짓말 난 그냥 참아내는 건데 바보 같은 남자라던 날 안아주는 건 이제야 넋이 나가 숨 쉬는 순간 정신을 못 차리고 알았어 난 그저 눈앞을 맴돌았지 점점 내 눈에 멀어져 가는걸 아무 의미 없단 걸 알게 됐어 그냥 미친 척했지 하긴 했지 뭔가 빠진 것처럼 날 꼬셔버린 시간을 버텨고 버틸 수 있게 된거야 난 그렇게 눈치를 챘던 짓궂은 장난말만 했었어 하지만 그걸 어쩌다 버렸어 제발 날 용서해 줬던 네가 눈가에 그저 눈빛이 보여 마치 내 마음속 깊이 박힌 눈빛으로 날 아프게 한 켠에 앞이 보이던 사람이 바로 너란걸 내가 다 봤어 우린 그렇게 난 눈치가 안 났지 그저 곁에 있잖아 나는 좀 미쳐버렸지 좀 늦은 저녁을 보냈던 중이었지 다신 볼 수 없게 널 그만 붙잡고버려 뒀지 난 좀 미쳤다 다시 끌려왔지 제자리에 멈춰서 버렸던 거야 나 혼자 죗악몽에 빠진 것 같은 짓을 했어 다뤘지 또 내 탓이겠지 되버린 탓이라며 날 미치게 보고 있는 게 더 나쁜 짓밟아가는 건 못됐지 나 스스로 다 너와 내 인생 최악의 모습인거지 말해본 적도 없는데 그냥 나쁜 모습 자고 또 다시 돌아온다는 말만 해본적이었어 그저 내버렸던 인생이야 다시 돌아올 거라며 날 버튼 박힌 잔인한 척 했다 나쁘게 보내며 계속 너를 보내주던 대로 걸어가는 패놓고 나만 있었어 정말 못되게 자빠진 않으니까 무뎌졌어 아무래도 내 길을 걸었다 고갤 떨리는 건 모두 다 잊고 있더라고 말야 진심해 난 좋은 일만 없었지 뭐가면 돼버린 시간만 또 끌려 보낸적이라던 내가 잘 버틴 게 나아졌지려다던 지난 밤새 술 한숨이였어 좋은 남자애태우린 사람아있는 패인 걸 다
3: 난 나쁜 하루를 보냈지 그 밤은 깊지 않은 척 넌 알 수 있는 내 눈에서 그게 보일까 봐 난 눈이 멀었나 봐 그냥 멍청했었지 내가 좀 봐서 눈치도 안 빠질 것 같아 아마 내겐 들리지 않는걸 나 혼자서 괜히 애태우는 건 좀 약이었나봐 잠시 스쳐가는 인연이길 바랬어 아픈 상처만 내게 남겨진다는 거짓말 난 그냥 참아내는 건데 바보 같은 남자라던 날 안아주는 건 이제야 넋이 나가 숨 쉬는 순간 정신을 못 차리고 알았어 난 그저 눈앞을 맴돌았지 점점 내 눈에 멀어져 가는걸 아무 의미 없단 걸 알게 됐어 그냥 미친 척했지 하긴 했지 뭔가 빠진 것처럼 날 꼬셔버린 시간을 버텨고 버틸 수 있게 된거야 난 그렇게 눈치를 챘던 짓궂은 장난말만 했었어 하지만 그걸 어쩌다 버렸어 제발 날 용서해 줬던 네가 눈가에 그저 눈빛이 보여 마치 내 마음속 깊이 박힌 눈빛으로 날 아프게 한 켠에 앞이 보이던 사람이 바로 너란걸 내가 다 봤어 우린 그렇게 난 눈치가 안 났지 그저 곁에 있잖아 나는 좀 미쳐버렸지 좀 늦은 저녁을 보냈던 중이었지 다신 볼 수 없게 널 그만 붙잡고버려 뒀지 난 좀 미쳤다 다시 끌려왔지 제자리에 멈춰서 버렸던 거야 나 혼자 죗악몽에 빠진 것 같은 짓을 했어 다뤘지 또 내 탓이겠지 되버린 탓이라며 날 미치게 보고 있는 게 더 나쁜 짓밟아가는 건 못됐지 나 스스로 다 너와 내 인생 최악의 모습인거지 말해본 적도 없는데 그냥 나쁜 모습 자고 또 다시 돌아온다는 말만 해본적이었어 그저 내버렸던 인생이야 다시 돌아올 거라며 날 버튼 박힌 잔인한 척 했다 나쁘게 보내며 계속 너를 보내주던 대로 걸어가는 패놓고 나만 있었어 정말 못되게 자빠진 않으니까 무뎌졌어 아무래도 내 길을 걸었다 고갤 떨리는 건 모두 다 잊고 있더라고 말야 진심해 난 좋은 일만 없었지 뭐가면 돼버린 시간만 또 끌려 보낸적이라던 내가 잘 버틴 게 나아졌지려다던 지난 밤새 술 한숨이였어 좋은 남자애태우린 사람아있는 패인걸 제
4: 난 나쁜 하루를 보냈지 그 밤은 깊지 않은 척 넌 알 수 있는 내 눈에서 그게 보일까 봐 난 눈이 멀었나 봐 그냥 멍청했었지 내가 좀 봐서 눈치도 안 빠질 것 같아 아마 내겐 들리지 않는걸 나 혼자서 괜히 애태우는 건 좀 약이었나봐 잠시 스쳐가는 인연이길 바랬어 아픈 상처만 내게 남겨진다는 거짓말 난 그냥 참아내는 건데 바보 같은 남자라던 날 안아주는 건 이제야 넋이 나가 숨 쉬는 순간 정신을 못 차리고 알았어 난 그저 눈앞을 맴돌았지 점점 내 눈에 멀어져 가는걸 아무 의미 없단 걸 알게 됐어 그냥 미친 척했지 하긴 했지 뭔가 빠진 것처럼 날 꼬셔버린 시간을 버텨고 버틸 수 있게 된거야 난 그렇게 눈치를 챘던 짓궂은 장난말만 했었어 하지만 그걸 어쩌다 버렸어 제발 날 용서해 줬던 네가 눈가에 그저 눈빛이 보여 마치 내 마음속 깊이 박힌 눈빛으로 날 아프게 한 켠에 앞이 보이던 사람이 바로 너란걸 내가 다 봤어 우린 그렇게 난 눈치가 안 났지 그저 곁에 있잖아 나는 좀 미쳐버렸지 좀 늦은 저녁을 보냈던 중이었지 다신 볼 수 없게 널 그만 붙잡고버려 뒀지 난 좀 미쳤다 다시 끌려왔지 제자리에 멈춰서 버렸던 거야 나 혼자 죗악몽에 빠진 것 같은 짓을 했어 다뤘지 또 내 탓이겠지 되버린 탓이라며 날 미치게 보고 있는 게 더 나쁜 짓밟아가는 건 못됐지 나 스스로 다 너와 내 인생 최악의 모습인거지 말해본 적도 없는데 그냥 나쁜 모습 자고 또 다시 돌아온다는 말만 해본적이었어 그저 내버렸던 인생이야 다시 돌아올 거라며 날 버튼 박힌 잔인한 척 했다 나쁘게 보내며 계속 너를 보내주던 대로 걸어가는 패놓고 나만 있었어 정말 못되게 자빠진 않으니까 무뎌졌어 아무래도 내 길을 걸었다 고갤 떨리는 건 모두 다 잊고 있더라고 말야 진심해 난 좋은 일만 없었지 뭐가면 돼버린 시간만 또 끌려 보낸적이라던 내가 잘 버틴 게 나아졌지려다던 지난 밤새 술 한숨이였어 좋은 남자애태우린 사람아있는 패인 걸 버
====================================================================================================
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
You left me to burn The ashes fall like the rain that comes down Lesson learned When I fall I fall so hard till Im laid out Looking around You gotta a smile just like a pyromaniac What Ive learned Sometimes you gotta let it all play out
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 넌 날 불에 태웠어 불타는 빗물처럼 흘러가는 내 어린 시절 처럼 일어나서 내가 쓰러질 때까지 배웠어 난 열심히 노래했어 네가 그곳에 누워 있을 때 난 미소 지을 수 있었어 가끔씩 떠다니다가
1: 넌 날 불에 태웠어 불타는 빗물처럼 흘러가는 내 어린 시절 처럼 일어나서 내가 쓰러질 때까지 배웠어 난 열심히 노래했어 네가 주변에 있을 때 난 미소 지을 거야 가끔씩 떠다니던 짓을
2: 넌 날 불에 태웠어 불타는 빗물처럼 흘러가는 내 어린 시절 처럼 일어나서 내가 쓰러질 때까지 배웠어 난 열심히 노래했어 네가 그곳에 누워 있을 때 난 미소 지을 수 있었어 가끔씩 떠다니던
3: 넌 날 불에 태웠어 불타는 빗물처럼 흘러가는 내 어린 시절 처럼 일어나서 내가 쓰러질 때까지 배웠어 난 열심히 노래했어 네가 주변에 있을 때 난 미소 지을 거야 가끔씩 떠다니다가는 짓
4: 넌 날 불에 태웠어 불타는 빗물처럼 흘러가는 내 어린 시절 처럼 일어나서 내가 쓰러질 때까지 배웠어 난 열심히 노래했어 네가 그곳에 누워 있을 때 난 미소 지을 거야 가끔씩 떠다니던 짓
====================================================================================================
loading configuration file ./checkpoints/asede-v2_ep10_lr0.0001_773/config.json
Model config EncoderDecoderConfig {
  "architectures": [
    "EncoderDecoderModel"
  ],
  "decoder": {
    "_name_or_path": "skt/kogpt2-base-v2",
    "_num_labels": 1,
    "activation_function": "gelu_new",
    "add_cross_attention": true,
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "author": "Heewon Jeon(madjakarta@gmail.com)",
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "created_date": "2021-04-28",
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "early_stopping": false,
    "embd_pdrop": 0.1,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 1,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "gradient_checkpointing": false,
    "id2label": {
      "0": "LABEL_0"
    },
    "initializer_range": 0.02,
    "is_decoder": true,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0
    },
    "layer_norm_epsilon": 1e-05,
    "length_penalty": 1.0,
    "license": "CC-BY-NC-SA 4.0",
    "max_length": 20,
    "min_length": 0,
    "model_type": "gpt2",
    "n_ctx": 1024,
    "n_embd": 768,
    "n_head": 12,
    "n_inner": null,
    "n_layer": 12,
    "n_positions": 1024,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 3,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "reorder_and_upcast_attn": false,
    "repetition_penalty": 1.0,
    "resid_pdrop": 0.1,
    "return_dict": true,
    "return_dict_in_generate": false,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "sep_token_id": null,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "task_specific_params": {
      "text-generation": {
        "do_sample": true,
        "max_length": 50
      }
    },
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": true,
    "vocab_size": 51200
  },
  "decoder_start_token_id": 1,
  "encoder": {
    "_name_or_path": "distilbert-base-uncased",
    "activation": "gelu",
    "add_cross_attention": false,
    "architectures": [
      "DistilBertForMaskedLM"
    ],
    "attention_dropout": 0.1,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "dim": 768,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.1,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_dim": 3072,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_range": 0.02,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 512,
    "min_length": 0,
    "model_type": "distilbert",
    "n_heads": 12,
    "n_layers": 6,
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 0,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "qa_dropout": 0.1,
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "seq_classif_dropout": 0.2,
    "sinusoidal_pos_embds": false,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_weights_": true,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.18.0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "vocab_size": 30522
  },
  "is_encoder_decoder": true,
  "model_type": "encoder-decoder",
  "pad_token_id": 1,
  "torch_dtype": "float32",
  "transformers_version": null
}
loading weights file ./checkpoints/asede-v2_ep10_lr0.0001_773/pytorch_model.bin
All model checkpoint weights were used when initializing EncoderDecoderModel.
All the weights of EncoderDecoderModel were initialized from the model checkpoint at ./checkpoints/asede-v2_ep10_lr0.0001_773.
If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.
====================================================================================================
Input:
Uh, Im higher than a plane and I dont wanna land Every time I speak I see a hundred grand Rollin so much weed its like the motherland I aint talkin English, Im talkin rubber bands
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Output:
0: 비행기를 타고 하늘보다 높이 올라가고 싶지 않아 내가 말할 때마다 수백 번을 말이야 그것은 아주 많은 다이아몬드 반지를 보는 중이야 나는 말해, "우리가 모르는 사이더라도 말야" (드디어?) 내버려,
1: 비행기를 타고 하늘보다 높이 올라가고 싶지 않아 내가 말할 때마다 수백 번을 말이야 그것은 아주 많은 다이아몬드 반지를 보는 중이야 나는 말해, "우리가 모르는 사이더라도 말야" (드디어 말하고 있어) 내 몸에 박
2: 비행기를 타고 하늘보다 높이 올라가고 싶지 않아 내가 말할 때마다 수백 번을 말이야 그것은 아주 많은 다이아몬드 반지를 보는 중이야 나는 말해, "우리가 모르는 사이더라도 말야" (드디어 말하고 있어) 내 언어들을
3: 비행기를 타고 하늘보다 높이 올라가고 싶지 않아 내가 말할 때마다 수백 번을 말이야 그것은 아주 많은 다이아몬드 반지를 보는 중이야 나는 말해, "우리가 모르는 사이더라도 말야" (드디어~) 플라스틱 컵을 들고
4: 비행기를 타고 하늘보다 높이 올라가고 싶지 않아 내가 말할 때마다 수백 번을 말이야 그것은 아주 많은 다이아몬드 반지를 보는 중이야 나는 말해, "우리가 모르는 사이더라도 말야" (드디어?) 내버려 두어

The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'.
The class this function is called from is 'PreTrainedTokenizerFast'.
asedskt/kogpt2-bas_ep10_lr0.0001_754
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Í¥úÏ∞ÆÏßÄÎßå Í¥úÏ∞ÆÏßÄ ÏïäÏïÑ ÏùµÏàôÌïòÎã§Í≥† ÌòºÏû£ÎßêÌñàÏßÄÎßå Îäò Ï≤òÏùåÏù∏ Í≤ÉÏ≤òÎüº ÏïÑÌåå', "I'm okay but I'm not okay I told myself I'm used to it But I'm in pain like it's the first time"]
['ÏñºÎßàÌÅº ÎÇòÎ•º Î≤ÑÎ†§Ïïº ÌïòÎãà ÏÑúÎüΩÍ≤å Ïö∞Îäî ÎÇ¥ Îßò Îì§Î¶¨Îãà ÎÑê ÎØ∏Ïπú ÎìØ ÏïàÍ≥† Ïã∂ÏùÄÎç∞ ÎÑà ÏóÜÏù¥ ÎÇò Í≤¨ÎîîÏßÄ Î™ªÌï¥', 'How much more do I have to throw myself away? Can you hear my sobbing heart? I want to hug you like crazy I can‚Äôt stand it without you']
{'input_ids': [101, 100, 100, 100, 1463, 30019, 30020, 29997, 30014, 30020, 30005, 30006, 29993, 30006, 29991, 30011, 100, 1456, 30017, 30022, 1465, 30008, 29999, 30017, 30023, 29999, 30019, 30021, 100, 1463, 30006, 30004, 30006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [10054, 382, 451, 10288, 449, 14778, 13726, 14197, 10054, 382, 451, 14415, 13650, 10288, 449, 14778, 10054, 10448, 27149, 11849, 22070, 10630, 444, 10054, 382, 451, 739, 10567, 11517, 20938, 739, 10455, 9707, 14197, 10054, 382, 451, 13799, 11553, 22495, 13008, 21964, 443, 739, 10455, 16239, 11370, 13063, 13019, 13394, 739, 9768, 21135]}
['[CLS]', '[UNK]', '[UNK]', '[UNK]', '·Ñã', '##·Öµ', '##·Ü®', '##·Ñâ', '##·ÖÆ', '##·Ü®', '##·Ñí', '##·Ö°', '##·ÑÉ', '##·Ö°', '##·ÑÄ', '##·Ö©', '[UNK]', '·ÑÇ', '##·Ö≥', '##·ÜØ', '·Ñé', '##·Ö•', '##·Ñã', '##·Ö≥', '##·Ü∑', '##·Ñã', '##·Öµ', '##·Ü´', '[UNK]', '·Ñã', '##·Ö°', '##·Ñë', '##·Ö°', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
['‚ñÅI', "'", 'm', '‚ñÅo', 'k', 'ay', '‚ñÅb', 'ut', '‚ñÅI', "'", 'm', '‚ñÅn', 'ot', '‚ñÅo', 'k', 'ay', '‚ñÅI', '‚ñÅt', 'old', '‚ñÅm', 'ys', 'el', 'f', '‚ñÅI', "'", 'm', '‚ñÅ', 'us', 'ed', '‚ñÅto', '‚ñÅ', 'it', '‚ñÅB', 'ut', '‚ñÅI', "'", 'm', '‚ñÅin', '‚ñÅp', 'ain', '‚ñÅl', 'ik', 'e', '‚ñÅ', 'it', "'s", '‚ñÅthe', '‚ñÅf', 'ir', 'st', '‚ñÅ', 'ti', 'me']
Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['transformer.h.7.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.3.crossattention.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.8.crossattention.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.11.crossattention.masked_bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.4.crossattention.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.7.crossattention.masked_bias', 'transformer.h.0.crossattention.bias', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.9.ln_cross_attn.weight', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.masked_bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.6.crossattention.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.9.crossattention.masked_bias', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.5.crossattention.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.11.crossattention.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.9.crossattention.bias', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.2.crossattention.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.7.crossattention.bias', 'transformer.h.10.crossattention.masked_bias', 'transformer.h.8.crossattention.masked_bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.10.crossattention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using amp half precision backend
/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 12088
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 256
  Gradient Accumulation steps = 64
  Total optimization steps = 470
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
asedskt/kogpt2-bas_ep10_lr0.0001_754
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
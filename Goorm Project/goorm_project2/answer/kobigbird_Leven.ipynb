{"cells":[{"cell_type":"markdown","metadata":{"id":"4V1iy8jrUCQR"},"source":["# Requirments"]},{"cell_type":"markdown","metadata":{"id":"wjNc9QiYxTgK"},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1Fm4Dx_xTFK"},"outputs":[],"source":["import os\n","import random\n","import math\n","import csv\n","import json\n","from statistics import mean\n","from typing import List, Tuple, Dict, Any\n","import uuid\n","\n","from tqdm.notebook import tqdm\n","from easydict import EasyDict as edict\n","\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","import wandb\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.nn.utils import clip_grad_norm_\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","\n","from torchinfo import summary\n","\n","from transformers import ElectraModel, ElectraTokenizer, ElectraForQuestionAnswering, AutoModelForQuestionAnswering, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Xzif3OL_oAC"},"outputs":[],"source":["for name in 'models', 'submissions':\n","    os.makedirs(name, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"aZI1eMrCRjLY"},"source":["# Set Arguments, Hyper-parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649532146294,"user":{"displayName":"조현수","userId":"06575509224308139266"},"user_tz":-540},"id":"j3W659v9z1Vl","outputId":"a1aee9d4-9ff4-4877-b233-eb1a4fb557c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["kobigbird_v2_ep2_max1024_lr6e-05_228\n"]}],"source":["args = edict({'w_project': 'test_project',\n","              'w_entity': 'lee-jun-yup',\n","              'learning_rate': 6e-5,\n","              'batch_size': {'train': 256,\n","                             'eval': 4,\n","                             'test': 256},\n","              'accumulate': 64,\n","              'epochs': 2,\n","              'seed': 42,\n","              # 'model_name': 'monologg/koelectra-base-v3-discriminator',\n","              'model_name': 'monologg/kobigbird-bert-base',\n","              'max_length': 1024})\n","# args['NAME'] = ''f'koelectra_ep{args.epochs}_lr{args.learning_rate}_{random.randrange(0, 1024)}'\n","args['NAME'] = ''f'kobigbird_v2_ep{args.epochs}_max{args.max_length}_lr{args.learning_rate}_{random.randrange(0, 1024)}'\n","print(args.NAME)"]},{"cell_type":"markdown","metadata":{"id":"EitWXKJmRw1b","toc-hr-collapsed":true},"source":["# Initialize"]},{"cell_type":"markdown","metadata":{"id":"51HhCeCTTDw5"},"source":["## Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjvZz4C3HMKg","outputId":"d00dd54f-2041-44b5-cc63-964ec0b21244"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchohs1221\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OACaHe-L-P-c"},"outputs":[],"source":["wandb.init(project = args.w_project, entity = args.w_entity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kB3HPxSa-TIn"},"outputs":[],"source":["wandb.run.name = args.NAME\n","wandb.config.learning_rate = args.learning_rate\n","wandb.config.epochs = args.epochs\n","wandb.config.batch_size = args.batch_size"]},{"cell_type":"markdown","metadata":{"id":"F6uJSyQCSEoa"},"source":["## Seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9b17md7VKba"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","\n","seed_everything(args.seed)"]},{"cell_type":"markdown","metadata":{"id":"YbKj9juZVV7W","tags":[]},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sm8pjc33VKYg"},"outputs":[],"source":["# tokenizer = ElectraTokenizer.from_pretrained(args.model_name)\n","tokenizer = AutoTokenizer.from_pretrained(args.model_name)"]},{"cell_type":"markdown","metadata":{"id":"UEQiDROgVXmg","tags":[]},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":657,"referenced_widgets":["0458f51b68e345c481afcfedebd28da7","b3ee09dbae5b419aa5dca8280991917a","21aaf3c8d96d41c78ba0f9f9b3fb799e","c0ed98ff850f49cea8aaac5bdfd397f9","58706e9bade3490cad47b6a42f010da6","dcc54673c0c346b5a4ef3adacee3b7c6","34e6307be5cd4217a1f6daf995d58055","33462c9795984bc6893f33998611a748","77c8c54416f94386b69153897b83f2f0","c00888417b344194847378c9424cafd4","50f084ad7ed34f61b78cac60e521d5dc"]},"executionInfo":{"elapsed":15130,"status":"ok","timestamp":1649532173118,"user":{"displayName":"조현수","userId":"06575509224308139266"},"user_tz":-540},"id":"ZJ-003lak8sj","outputId":"f6b698bb-b693-42dd-f84b-1af91549ac75"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.pooler.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'bert.pooler.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BigBirdForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BigBirdForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BigBirdForQuestionAnswering were not initialized from the model checkpoint at monologg/kobigbird-bert-base and are newly initialized: ['qa_classifier.output.LayerNorm.bias', 'qa_classifier.qa_outputs.bias', 'qa_classifier.output.LayerNorm.weight', 'qa_classifier.intermediate.dense.bias', 'qa_classifier.intermediate.dense.weight', 'qa_classifier.output.dense.bias', 'qa_classifier.output.dense.weight', 'qa_classifier.qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# model = ElectraForQuestionAnswering.from_pretrained(args.model_name)\n","model = AutoModelForQuestionAnswering.from_pretrained(args.model_name)\n","# summary(model, (args.batch_size.train//args.accumulate, args.max_length), dtypes=['torch.IntTensor'], device='cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OoYuu2ckzJ6"},"outputs":[],"source":["model.cuda();"]},{"cell_type":"markdown","metadata":{"id":"9hCiOQO4VYqM","tags":[]},"source":["## Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TonS9AsIVQlv"},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"MlKUCHM9SUim","tags":[],"toc-hr-collapsed":true},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{"id":"-SPF2-SSVoT5"},"source":["## Load, Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoiHRyTOugMj"},"outputs":[],"source":["class KoMRC:\n","    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n","        self._data = data\n","        self._indices = indices\n","\n","\n","    # Json을 불러오는 메소드\n","    @classmethod\n","    def load(cls, file_path: str):\n","        with open(file_path, 'r', encoding='utf-8') as fd:\n","            data = json.load(fd)\n","\n","        indices = []\n","        for d_id, document in enumerate(data['data']):\n","            for p_id, paragraph in enumerate(document['paragraphs']):\n","                for q_id, _ in enumerate(paragraph['qas']):\n","                    indices.append((d_id, p_id, q_id))\n","        \n","        return cls(data, indices)\n","\n","    # Json을 불러오는 메소드\n","    @classmethod\n","    def loads(cls, *file_path: str):\n","        datas = {'data': []}\n","        indices = []\n","        \n","        for f in file_path:\n","            with open(f, 'r', encoding='utf-8') as fd:\n","                data = json.load(fd)\n","            datas['data'] += data['data']\n","            \n","        for d_id, document in enumerate(datas['data']):\n","            for p_id, paragraph in enumerate(document['paragraphs']):\n","                for q_id, _ in enumerate(paragraph['qas']):\n","                    indices.append((d_id, p_id, q_id))\n","\n","        return cls(datas, indices)\n","\n","    # 데이터 셋을 잘라내는 메소드\n","    @classmethod\n","    def split(cls, dataset, eval_ratio: float=.1):\n","        indices = list(dataset._indices)\n","        random.shuffle(indices)\n","        train_indices = indices[int(len(indices) * eval_ratio):]\n","        eval_indices = indices[:int(len(indices) * eval_ratio)]\n","\n","        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n","\n","\n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        d_id, p_id, q_id = self._indices[index]\n","        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n","\n","        qa = paragraph['qas'][q_id]\n","\n","        if 'guid' in qa:\n","            guid = qa['guid']\n","        else:\n","            guid = uuid.uuid4().hex\n","\n","        context = paragraph['context'].replace('\\n', 'n').replace('\\xad', ' ').replace('\\xa0', ' ').replace('\\u200b', ' ')\n","\n","        question = qa['question'].replace('\\n', 'n').replace('\\xad', ' ').replace('\\xa0', ' ').replace('\\u200b', ' ')\n","\n","        answers = qa['answers']\n","        if answers != None and answers != []:\n","            for a in answers:\n","                a['text'] = a['text'].replace('\\n', 'n').replace('\\xad', ' ').replace('\\xa0', ' ').replace('\\u200b', ' ')\n","\n","\n","        return {'guid': guid,\n","            'context': context,\n","            'question': question,\n","            'answers': answers\n","        }\n","\n","    def __len__(self) -> int:\n","        return len(self._indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1652,"status":"ok","timestamp":1649532185028,"user":{"displayName":"조현수","userId":"06575509224308139266"},"user_tz":-540},"id":"CDu4gwv_ugKW","outputId":"9f4f4a87-cd03-4fea-f8ee-3bbab3179d81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Samples: 12037 243425 100268\n"]}],"source":["dataset1 = KoMRC.load('./datasets2/train.json')\n","dataset2 = KoMRC.load('./datasets2/ko_nia_normal_squad_all.json')\n","dataset3 = KoMRC.load('./datasets2/ko_wiki_v1_squad.json')\n","print(\"Number of Samples:\", len(dataset1), len(dataset2), len(dataset3))"]},{"cell_type":"markdown","metadata":{"id":"AaAHVuoEVs32"},"source":["## Tokenize & Tag Token Positions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AluNiOayugGE"},"outputs":[],"source":["class TokenizedKoMRC(KoMRC):\n","    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n","        super().__init__(data, indices)\n","        self._tokenizer = tokenizer\n","\n","\n","    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n","        position = 0\n","        tokens = []\n","\n","        sentence_tokens = []\n","        for word in sentence.split():\n","            if '[UNK]' in tokenizer.tokenize(word):\n","                sentence_tokens.append(word)\n","            else:\n","                sentence_tokens += tokenizer.tokenize(word)\n","        \n","        for morph in sentence_tokens:\n","            if len(morph) > 2:\n","                if morph[:2] == '##':\n","                    morph = morph[2:]\n","\n","            position = sentence.find(morph, position)\n","            tokens.append((morph, (position, position + len(morph))))\n","            position += len(morph)\n","            \n","        return tokens\n","            \n","\n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        sample = super().__getitem__(index)\n","        # sample = {'guid': guid, 'context': context, 'question': question, 'answers': answers}\n","\n","        context, position = zip(*self._tokenize_with_position(sample['context']))\n","        context, position = list(context), list(position)\n","\n","        question = self._tokenizer.tokenize(sample['question'])\n","\n","        if sample['answers'] is not None:\n","            answers = []\n","            for answer in sample['answers']:\n","                for start, (position_start, position_end) in enumerate(position):\n","                    if position_start <= answer['answer_start'] < position_end:\n","                        break\n","                else:\n","                    print(context, answer)\n","                    print(answer['guid'])\n","                    print(answer['answer_start'])\n","                    raise ValueError(\"No mathced start position\")\n","\n","                target = ''.join(answer['text'].split(' '))\n","                source = ''\n","                for end, morph in enumerate(context[start:], start):\n","                    source += morph\n","                    if target in source:\n","                        break\n","                else:\n","                    print(context, answer)\n","                    print(answer['guid'])\n","                    print(answer['answer_start'])\n","                    raise ValueError(\"No Matched end position\")\n","\n","                answers.append({'start': start, 'end': end})\n","                \n","        else:\n","            answers = None\n","        \n","        return {\n","            'guid': sample['guid'],\n","            'context_original': sample['context'],\n","            'context_position': position,\n","            'question_original': sample['question'],\n","            'context': context,\n","            'question': question,\n","            'answers': answers\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":606,"status":"ok","timestamp":1649532185632,"user":{"displayName":"조현수","userId":"06575509224308139266"},"user_tz":-540},"id":"K3Cu4vU1ugD2","outputId":"0b7c594e-aebd-4782-c069-8b2f150d1ffa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Train Samples: 355730\n"]}],"source":["train_dataset = TokenizedKoMRC.loads('./datasets2/train.json', './datasets2/ko_nia_normal_squad_all.json', './datasets2/ko_wiki_v1_squad.json')\n","# train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\n","\n","print(\"Number of Train Samples:\", len(train_dataset))\n","# print(\"Number of Dev Samples:\", len(dev_dataset))\n","# print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"]},{"cell_type":"markdown","metadata":{"id":"5NQ6g-qhV_7g"},"source":["## Input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyN44Vg-uf-Y"},"outputs":[],"source":["class Indexer:\n","    def __init__(self, vocabs: List[str], max_length: int=args.max_length):\n","        self.max_length = max_length\n","        self.vocabs = vocabs\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocabs)\n","    @property\n","    def pad_id(self):\n","        return tokenizer.vocab['[PAD]']\n","    @property\n","    def unk_id(self):\n","        return tokenizer.vocab['[UNK]']\n","    @property\n","    def cls_id(self):\n","        return tokenizer.vocab['[CLS]']\n","    @property\n","    def sep_id(self):\n","        return tokenizer.vocab['[SEP]']\n","\n","\n","    def sample2ids(self, sample: Dict[str, Any],) -> Dict[str, Any]:\n","        context = [tokenizer.convert_tokens_to_ids(token) for token in sample['context']]\n","        question = [tokenizer.convert_tokens_to_ids(token) for token in sample['question']]\n","\n","        context = context[:self.max_length-len(question)-3]             # Truncate context\n","        \n","        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]\n","        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)\n","\n","        if sample['answers'] is not None:\n","            answer = sample['answers'][0]\n","            start = min(len(question) + 2 + answer['start'], self.max_length - 1)\n","            end = min(len(question) + 2 + answer['end'], self.max_length - 1)\n","        else:\n","            start = None\n","            end = None\n","\n","        return {\n","            'guid': sample['guid'],\n","            'context': sample['context_original'],\n","            'question': sample['question_original'],\n","            'position': sample['context_position'],\n","            'input_ids': input_ids,\n","            'token_type_ids': token_type_ids,\n","            'start': start,\n","            'end': end\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grrjgBHWuf8H"},"outputs":[],"source":["indexer = Indexer(list(tokenizer.vocab.keys()))"]},{"cell_type":"markdown","metadata":{"id":"6gLlDcdhWMVy"},"source":["## Attention Mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2nGyyJLuf54"},"outputs":[],"source":["class IndexerWrappedDataset:\n","    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:\n","        self._dataset = dataset\n","        self._indexer = indexer\n","\n","    def __len__(self) -> int:\n","        return len(self._dataset)\n","    \n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        sample = self._indexer.sample2ids(self._dataset[index])\n","        sample['attention_mask'] = [1] * len(sample['input_ids'])\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5ee_MDquf3p"},"outputs":[],"source":["indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\n","# indexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)"]},{"cell_type":"markdown","metadata":{"id":"BrnFWwNFWydX"},"source":["## Collate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pp4TTU5Oufy-"},"outputs":[],"source":["class Collator:\n","    def __init__(self, indexer: Indexer) -> None:\n","        self._indexer = indexer\n","\n","\n","    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n","        samples = {key: [sample[key] for sample in samples] for key in samples[0]}\n","\n","        for key in 'start', 'end':\n","            if samples[key][0] is None:\n","                samples[key] = None\n","            else:\n","                samples[key] = torch.tensor(samples[key], dtype=torch.long)\n","        \n","        for key in 'input_ids', 'attention_mask', 'token_type_ids':\n","            samples[key] = pad_sequence([torch.tensor(sample, dtype=torch.long) for sample in samples[key]],\n","                                        batch_first=True,\n","                                        padding_value=self._indexer.pad_id)\n","\n","        return samples"]},{"cell_type":"markdown","metadata":{"id":"uNj_C_6sSwpE"},"source":["## Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlVyogmMufwZ"},"outputs":[],"source":["collator = Collator(indexer)\n","train_loader = DataLoader(indexed_train_dataset,\n","                          batch_size = args.batch_size.train // args.accumulate,\n","                          shuffle = True,\n","                          collate_fn = collator,\n","                          num_workers = 2)\n","\n","# dev_loader = DataLoader(indexed_dev_dataset,\n","#                         batch_size = args.batch_size.eval,\n","#                         shuffle = False,\n","#                         collate_fn = collator,\n","#                         num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1649532186561,"user":{"displayName":"조현수","userId":"06575509224308139266"},"user_tz":-540},"id":"3lcvlUltufuC","outputId":"8bdf541f-6e03-41c0-fe6c-a6fdfe080842"},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["batch = next(iter(train_loader))\n","# batch = next(iter(dev_loader))\n","# print(batch['input_ids'])\n","# print(batch['input_ids'].shape)\n","# print(list(batch.keys()))"]},{"cell_type":"markdown","metadata":{"id":"m8bJH_DNRfVm"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"eAfQOTuPeuWN","toc-hr-collapsed":true},"source":["## Empty Cuda Cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WREvLj-AARp"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKKBfjduHMKo"},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"markdown","metadata":{"id":"8tkYfToDenKV","tags":[],"toc-hr-collapsed":true},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["3610c327a7534b03992d8b6b5f0f0203"]},"id":"Rfm5X-bEufpW","outputId":"b68c37ae-18c2-412e-8adb-37f3a9ecd227","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 ===============================================================================================================\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3610c327a7534b03992d8b6b5f0f0203","version_major":2,"version_minor":0},"text/plain":["HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=88933.0), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Attention type 'block_sparse' is not possible if sequence_length: 525 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"]}],"source":["train_losses = []\n","dev_losses = []\n","\n","train_loss = []\n","dev_loss = []\n","\n","loss_accumulate = 0.\n","\n","best_model = [-1, int(1e9)]\n","\n","for epoch in range(args.epochs):\n","    print(\"Epoch\", epoch, '===============================================================================================================')\n","\n","    # Train    \n","    progress_bar_train = tqdm(train_loader, desc='Train')\n","    for i, batch in enumerate(progress_bar_train, 1):\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        batch = {key: value.cuda() for key, value in batch.items()}\n","        \n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        output = model(**batch)\n","\n","        start_logits = output.start_logits\n","        end_logits = output.end_logits\n","        \n","        loss = (F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)) / args.accumulate\n","        loss.backward()\n","\n","        loss_accumulate += loss.item()\n","\n","        del batch, start, end, start_logits, end_logits, loss\n","        \n","        if i % args.accumulate == 0:\n","            # clip_grad_norm_(model.parameters(), max_norm=1.)\n","            optimizer.step()\n","            optimizer.zero_grad(set_to_none=False)\n","\n","            train_loss.append(loss_accumulate)\n","            progress_bar_train.set_description(f\"Train - Loss: {loss_accumulate:.3f}\")\n","            loss_accumulate = 0.\n","        else:\n","            continue\n","\n","        if i % int(len(train_loader) / (args.accumulate * 50)) == 0:\n","#             # Evaluation\n","#             for batch in dev_loader:\n","#                 del batch['guid'], batch['context'], batch['question'], batch['position']\n","#                 batch = {key: value.cuda() for key, value in batch.items()}\n","\n","#                 start = batch.pop('start')\n","#                 end = batch.pop('end')\n","                \n","#                 model.eval()\n","#                 with torch.no_grad():\n","#                     output = model(**batch)\n","                \n","#                     start_logits = output.start_logits\n","#                     end_logits = output.end_logits\n","#                 model.train()\n","\n","#                 loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","\n","#                 dev_loss.append(loss.item())\n","\n","#                 del batch, start, end, start_logits, end_logits, loss\n","\n","            train_losses.append(mean(train_loss))\n","            # dev_losses.append(mean(dev_loss))\n","            train_loss = []\n","            # dev_loss = []\n","\n","            \n","            # if dev_losses[-1] <= best_model[1]:\n","            #     best_model = (epoch, dev_losses[-1])\n","            if dev_losses[-1] <= best_model[1]:\n","                best_model = (epoch, dev_losses[-1])\n","                model.save_pretrained(f'models/{args.NAME}_{epoch}')\n","                # print(f'model saved!!\\nvalid_loss: {dev_losses[-1]}')\n","                \n","            wandb.log({\"train_loss\": train_losses[-1],\n","                       \"valid_loss\": dev_losses[-1]})\n","            \n","\n","    print(f\"Train Loss: {train_losses[-1]:.3f}\")\n","    print(f\"Valid Loss: {dev_losses[-1]:.3f}\")\n","    print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')"]},{"cell_type":"markdown","metadata":{"id":"5NeU3ogjS2LZ","tags":[]},"source":["## Visualize Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nDxYe7KufnH"},"outputs":[],"source":["plt.plot(train_losses, label=\"Train Loss\")\n","plt.plot(dev_losses, label=\"Dev Loss\")\n","plt.xlabel(\"Step\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wJKrztCmXRBB"},"source":["# Test"]},{"cell_type":"markdown","metadata":{"id":"r4vjm5TrYP-S"},"source":["## Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5f9hR0qsu9Ap"},"outputs":[],"source":["test_dataset = TokenizedKoMRC.load('./datasets2/test.json')\n","indexer_test = Indexer(list(tokenizer.vocab.keys()))\n","indexed_test_dataset = IndexerWrappedDataset(test_dataset, indexer_test)\n","print(\"Number of Test Samples\", len(test_dataset))\n","# print(test_dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"y8js2FfYYJR7"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9rSEewjHMKq"},"outputs":[],"source":["best_model[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nESvW3pKufkv"},"outputs":[],"source":["model = AutoModelForQuestionAnswering.from_pretrained(f'models/{args.NAME}_{best_model[0]}')\n","model.cuda();\n","# summary(model, (args.batch_size.train//args.accumulate, args.max_length), dtypes=['torch.IntTensor'], device='cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbM5uvjDufiJ"},"outputs":[],"source":["for idx, sample in zip(range(1, 4), indexed_train_dataset):\n","    print(f'------{idx}------')\n","    print('Context:', sample['context'])\n","    print('Question:', sample['question'])\n","    \n","    input_ids, token_type_ids = [\n","        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","        for key in (\"input_ids\", \"token_type_ids\")\n","    ]\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        output = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","\n","    start_logits = output.start_logits\n","    end_logits = output.end_logits\n","    start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","\n","    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","\n","    index = torch.argmax(probability).item()\n","    \n","    start = index // len(end_prob)\n","    end = index % len(end_prob)\n","    \n","    start_str = sample['position'][start][0]\n","    end_str = sample['position'][end][1]\n","\n","    print('Answer:', sample['context'][start_str:end_str])"]},{"cell_type":"markdown","metadata":{"id":"Ul-FlUBZY88_"},"source":["## Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cyVG3J2hu8-s"},"outputs":[],"source":["start_visualize = []\n","end_visualize = []\n","\n","with torch.no_grad(), open(f'submissions/{args.NAME}_00_2.csv', 'w') as fd:\n","    writer = csv.writer(fd)\n","    writer.writerow(['Id', 'Predicted'])\n","\n","    rows = []\n","    # for sample in tqdm(test_dataset, \"Testing\"):\n","    for sample in tqdm(indexed_test_dataset, \"Testing\"):\n","        input_ids, token_type_ids = [torch.tensor(sample[key], dtype=torch.long, device=\"cuda\") for key in (\"input_ids\", \"token_type_ids\")]\n","        # print(sample)\n","    \n","        model.eval()\n","        with torch.no_grad():\n","            output = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","\n","        start_logits = output.start_logits\n","        end_logits = output.end_logits\n","        start_logits.squeeze_(0), end_logits.squeeze_(0)\n","\n","        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","\n","        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","\n","        # 토큰 길이 8까지만\n","        for row in range(len(start_prob) - 8):\n","            probability[row] = torch.cat((probability[row][:8+row].cpu(), torch.Tensor([0] * (len(start_prob)-(8+row))).cpu()), 0)\n","\n","        index = torch.argmax(probability).item()\n","\n","        start = index // len(end_prob)\n","        end = index % len(end_prob)\n","        \n","        # 확률 너무 낮으면 자르기\n","        if start_prob[start] >= 0 or end_prob[end] >= 0:\n","            start_str = sample['position'][start][0]\n","            end_str = sample['position'][end][1]\n","        else:\n","            start_str = 0\n","            end_str = 0\n","\n","        start_visualize.append((list(start_prob.cpu()), (start, end), (start_str, end_str)))\n","        end_visualize.append((list(end_prob.cpu()), (start, end), (start_str, end_str)))\n","        \n","        rows.append([sample[\"guid\"], sample['context'][start_str:end_str]])\n","\n","    writer.writerows(rows)"]},{"cell_type":"markdown","metadata":{"id":"tess2OmJba2K"},"source":["## Visualize Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBsEFEOxbRt4"},"outputs":[],"source":["idx = 0\n","\n","start_visualize = np.array(start_visualize)\n","end_visualize = np.array(end_visualize)\n","\n","start_probalilities, token_pos, str_pos = start_visualize[:,0], start_visualize[:,1], start_visualize[:,2]\n","end_probalilities, token_pos, str_pos = end_visualize[:,0], end_visualize[:,1], end_visualize[:,2]\n","\n","plt.plot(start_probalilities[idx], label=\"start probability\")\n","plt.plot(end_probalilities[idx], label=\"end probability\")\n","plt.xlabel(\"context token index\")\n","plt.ylabel(\"probablilty\")\n","plt.legend()\n","plt.show()\n","\n","print('token position:', token_pos[idx])\n","print('context position:', str_pos[idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r71Z1hRUHMKr"},"outputs":[],"source":["for i, (start, end) in enumerate(token_pos):\n","    if end - start > 1:\n","        if i > 0:\n","            plt.plot(start_probalilities[i])\n","            plt.plot(end_probalilities[i])\n","            print(i, start, end)\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"py54fr9EHMKr"},"outputs":[],"source":["temp = []\n","h = 0\n","l = 100\n","for i, (start, end) in enumerate(token_pos):\n","    h = max(h, end - start)\n","    l = min(l ,end - start)\n","    temp.append(end - start)\n","plt.plot(temp)\n","print(mean(temp))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oM9ByDmjHMKr"},"outputs":[],"source":["mu = mean(temp)\n","sigma = math.sqrt(np.var(temp))\n","x = np.linspace(-100, 100, len(temp))\n","g = (1 / np.sqrt(2*np.pi * sigma**2)) * np.exp(- (x-mu)**2 / (2*sigma**2))\n","plt.title('Gaussian')\n","plt.plot(x, g)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Or5GnxpHMKr"},"outputs":[],"source":["z = [(i-mu)/sigma for i in temp]\n","print(f'평균: {round(mean(z), 9)}')\n","print(f'표준편차: {math.sqrt(np.var(z))}')\n","print('-----90%------')\n","print(mu - 1.645*sigma/math.sqrt(len(temp)))\n","print(mu + 1.645*sigma/math.sqrt(len(temp)))\n","print('-----95%------')\n","print(mu - 1.96*sigma/math.sqrt(len(temp)))\n","print(mu + 1.96*sigma/math.sqrt(len(temp)))\n","print('-----99%------')\n","print(mu - 2.576*sigma/math.sqrt(len(temp)))\n","print(mu + 2.576*sigma/math.sqrt(len(temp)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG3zJMMtHMKr"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["F6uJSyQCSEoa","YbKj9juZVV7W","9hCiOQO4VYqM","MlKUCHM9SUim","eAfQOTuPeuWN"],"machine_shape":"hm","name":"kobigbird_Leven.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0458f51b68e345c481afcfedebd28da7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3ee09dbae5b419aa5dca8280991917a","IPY_MODEL_21aaf3c8d96d41c78ba0f9f9b3fb799e","IPY_MODEL_c0ed98ff850f49cea8aaac5bdfd397f9"],"layout":"IPY_MODEL_58706e9bade3490cad47b6a42f010da6"}},"21aaf3c8d96d41c78ba0f9f9b3fb799e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33462c9795984bc6893f33998611a748","max":451741507,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77c8c54416f94386b69153897b83f2f0","value":451741507}},"33462c9795984bc6893f33998611a748":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34e6307be5cd4217a1f6daf995d58055":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50f084ad7ed34f61b78cac60e521d5dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58706e9bade3490cad47b6a42f010da6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77c8c54416f94386b69153897b83f2f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3ee09dbae5b419aa5dca8280991917a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcc54673c0c346b5a4ef3adacee3b7c6","placeholder":"​","style":"IPY_MODEL_34e6307be5cd4217a1f6daf995d58055","value":"Downloading: 100%"}},"c00888417b344194847378c9424cafd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0ed98ff850f49cea8aaac5bdfd397f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c00888417b344194847378c9424cafd4","placeholder":"​","style":"IPY_MODEL_50f084ad7ed34f61b78cac60e521d5dc","value":" 431M/431M [00:08&lt;00:00, 67.7MB/s]"}},"dcc54673c0c346b5a4ef3adacee3b7c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}